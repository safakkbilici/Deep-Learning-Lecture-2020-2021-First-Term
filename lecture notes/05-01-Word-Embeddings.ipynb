{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"text-align:center\"> <img src=\"i_could_care_less.png\" width=\"700\" height=\"700\"> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from IPython.display import Image\n",
    "#Image(filename='i_could_care_less.png') \n",
    "#<div style=\"text-align:center\"> <img src=\"i_could_care_less.png\" width=\"700\" height=\"700\"> </div>\n",
    "from IPython.display import Image\n",
    "from IPython.display import HTML\n",
    "html1 = '<div style=\"text-align:center\"> <img src=\"i_could_care_less.png\" width=\"700\" height=\"700\"> </div>'\n",
    "HTML(html1)\n",
    "\n",
    "#from IPython.display import Image\n",
    "#from IPython.core.display import HTML \n",
    "#Image(filename= \"i_could_care_less.png\",width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representation With One-Hot Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is the most common, most basic way to turn a token into a vector. It consists in associating a unique integer index to every word, then turning this integer index $i$ into a binary vector of size $V$, which is the size of our vocabulary, that would be all-zeros except for the $i$-th entry, which would be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1\n",
    "\n",
    "sentence = \"the cat ate the god\"\n",
    "seq = []\n",
    "for word in sentence.split():\n",
    "    one_hot = np.zeros((1,len(token_index)))\n",
    "    if word not in token_index:\n",
    "        seq.append(one_hot.squeeze())\n",
    "    else:\n",
    "        one_hot[0,token_index[word]-1] = one_hot[0,token_index[word]-1] + 1\n",
    "        seq.append(one_hot.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
       " array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open('cats100.test.txt') as file_handler:\n",
    "    data = file_handler.read()\n",
    "\n",
    "token_idx = {}\n",
    "for word in data.split():\n",
    "    if word not in token_idx:\n",
    "        token_idx[word] = len(token_idx) + 1\n",
    "\n",
    "sentence = \"Geçtiğimiz zamanlarda kötü olaylar yaşandı\"\n",
    "one_hot_sentence = np.zeros((len(sentence.split()),len(token_idx)),dtype=np.int32)\n",
    "for word_idx,word in enumerate(sentence.split()):\n",
    "    one_hot_sentence[word_idx,token_idx[word] - 1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"one-hot.png\" alt=\"Trulli\" width=\"300\" height=\"300\">\n",
    "  <figcaption>Figure is taken from <a href=\"https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\">Stanford cs-230 cheatsheet</a> </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " There are two major issues with this approach. First issue is the curse of dimensionality, which refers to all sorts of problems that arise with data in high dimensions. This requires exponentially large memory space. Most of the matrix is taken up by zeros, so useful data becomes sparse. Imagine we have a vocabulary of 50,000. (There are roughly a million words in English language.) Each word is represented with 49,999 zeros and a single one, and we need 50,000 squared = 2.5 billion units of memory space. Not computationally efficient.\n",
    "   \n",
    "\n",
    "Second issue is that every one hotted vector is orthogonal to each other. You cannot measure the\n",
    "similarity (like cosine similarity) on this vectors.\n",
    "\n",
    "\n",
    "Those vectors can be visualized in 3 or 2-dimensional space as an example. In $\\mathbb{R}^3$ (in other words, vocabulary size of 3, $\\mid V \\mid=3$), the word vectors become our span set which is \n",
    "$\\left\\{\\begin{bmatrix} 1\\\\0\\\\0\\end{bmatrix},\n",
    "\\begin{bmatrix} 0\\\\1\\\\0\\end{bmatrix},\n",
    "\\begin{bmatrix} 0\\\\0\\\\1\\end{bmatrix}\\right\\}$. The span set is linearly independent set. Therefore we cannot compute similarity metrics. The vectors $\\vec{u_1}, \\vec{u_2}, \\vec{u_3}$ are orthogonal. Similarity metrics gives the result of zero. For example the cosine similarity:\n",
    "<br>\n",
    "    <br>\n",
    "    \n",
    "\n",
    "$$ \\text{cos-sim}(\\vec{u_1},\\vec{u_2}) = \\frac{\\langle \\vec{u_1}, \\vec{u_2} \\rangle}{\\lVert \\vec{u_1} \\lVert_2 \\times \\lVert \\vec{u_2} \\lVert_2 } = \\frac{\\sum_{i=0}^n u_{1_i} \\times u_{2_i}}{\\sqrt{\\sum_{i=0}^n u_{1_i}^2} \\times \\sqrt{\\sum_{i=0}^n u_{2_i}^2}} = \\frac{1 \\times 0 + 0 \\times 1 + 0 \\times 0}{1 + 1} = 0$$\n",
    "<br>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how to deal with those problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Semantics And Distributional Linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Words that occur in <em>similar contexts</em> tend to have similar meanings. The link between similarity and words are distributed and similarity in what they mean is called <strong>distributional hypothesis</strong> or <strong>distributional semantics</strong> in the field of Computational Linguistics. So what can be counted when we say similar contexts? For example if you surf on the Wikipedia page of linguistics, the words in this page somehow related with each other in the context of linguistics. This was formulated firsty by Martin Joos (Description of Language Design, 1950), Zellig Harris (Distributional Structure, 1954), John Rupert Firth (Applications of General Linguistics, 1957)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Some words have similar meanings, for example word cat and dog <strong>similar</strong>. Also, words can be <strong>antonyms</strong>, for example hot and cold. And words have <strong>connotations</strong> (TR: çağrışım), for example happy->positive connotation and sad->negative connotation. Can you feel the similarity of words, [study, exam, night, FF]?\n",
    "\n",
    "\n",
    "Also each word can have multiple meanings. The word mouse can refeer to the rodent or the cursor control device. We call each of these aspects of the meaning of mouse a <strong>word sense</strong>. In other words words can be <strong>polysemous</strong> (have multiple senses), which can lead us to make word interpretations difficult! \n",
    "<br> <br>\n",
    "\n",
    "- <strong>Word Sense Disambiguation</strong>: \"Mouse info\" (person who types this into a web search engine, looking for a pet info or a tool?) (determining which sense of a word is being used in a particular context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word **similarity** is very useful in larger semantics tasks. Knowing how similar two words are can help in computing how similar the meaning of two phrases or sentences are, a very important component of natural language understanding tasks like **question answering**, **summarization** etc.\n",
    "\n",
    "| Word1       | Word2       |  Similarity (0-10) |\n",
    "| ----------- | ----------- |  -----------       |\n",
    "| Vanish      | Disappear   |  9.8             |\n",
    "| Behave      | Obey        |  7.3              |\n",
    "| Belief      | Impression  |  5.95              |\n",
    "| Muscle      | Bone        |  3.65              |\n",
    "| Modest      | Flexible    |  0.98              |\n",
    "| Hole        | Agreement   |  0.3              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should look at **word relatedness** , the meaning of two words can be related in ways other than similarity. One such class of connections is called word **relatedness**, also traditionally called word **association** in pysch.\n",
    "\n",
    "- The word *cup* and *coffee*.\n",
    "- The word *inzva* and *deep learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also words can affective meanings. Osgood et al. 1957, proposed that words varied along three important dimensions of affective meaning: *valence, arousal, dominance*.\n",
    "- **valence**: the pleasantness of the stimulus.\n",
    "- **arousal**: the intensity of emotion provoked by the stimulus.\n",
    "- **dominance**: the degree of control exerted by the stimulus.\n",
    "\n",
    "Examples: \n",
    "- happy(1) $\\uparrow$, satisfied(1) $\\uparrow$; annoyed(1) $\\downarrow$, unhappy(1)$\\downarrow$\n",
    "- excited(2) $\\uparrow$, frenzied(2) $\\uparrow$; relaxed(1) $\\downarrow$, calm(2) $\\downarrow$\n",
    "- important(3)$\\uparrow$, controlling(3)$\\uparrow$; awed(3)$\\downarrow$, influenced(3)$\\downarrow$\n",
    "\n",
    "*Question: does word embeddings has these dimesions?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we build a computational model that successfully deals with the different aspects of word meaning we saw above (word senses, word similarity, word relatedness, connotation etc.)?\n",
    "\n",
    "**Instead of representing words with one-hot vector, sparse; with word embeddings we represents words with dense vectors**. \n",
    "\n",
    "**The idea of vector semantics is thus to represent a word as a point in some multidimensional semantic space.** Vectors for representing words are generally called **embeddings**. We **LEARN**  this embeddings form an arbitrary context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<figure>\n",
    "  <img src=\"embed-dims.png\" alt=\"Trulli\" width=\"400\" height=\"400\">\n",
    "  <figcaption>Figure is taken from <a href=\"https://medium.com/@hari4om/word-embedding-d816f643140\">this medium post</a> </figcaption>\n",
    "</figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main two advantages of this word embeddings are:\n",
    "\n",
    "- Now we represent words with dense vectors, which leads low memory requirements.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"embed2.png\" alt=\"Trulli\" width=\"300\" height=\"300\">\n",
    "  <figcaption>Figure is taken from <a href=\"https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\">Stanford cs-230 cheatsheet</a> </figcaption>\n",
    "</figure>\n",
    "\n",
    "- We can now calculate similarity metrics on these vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Word Embeddings\n",
    "\n",
    "Word embeddings can be visualized with various dimensionality reduction/matrix factorization algorithms.\n",
    "\n",
    "<p float=\"left\">\n",
    "\n",
    "  <img src=\"socher.png\" width=\"600\" />\n",
    "      \n",
    "  <img src=\"tsne2.png\" width=\"1000\" /> \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- left-figure source: [Zero-Shot Learning Through Cross-Modal Transfer (Socher et al., 2013, NeurIPS)](https://nlp.stanford.edu/~socherr/SocherGanjooManningNg_NIPS2013.pdf)\n",
    "- right-figure source: [Word representations: A simple and general method for semi-supervised learning (Turian et al., 2010)](http://metaoptimize.s3.amazonaws.com/cw-embeddings-ACL2010/embeddings-mostcommon.EMBEDDING_SIZE=50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "  <img style=\"padding: 90px\" src=\"colobert.png\" width=\"800\" />\n",
    "      \n",
    "  <img style=\"padding: 80px\" src=\"bilingual.png\" width=\"500\" /> \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- left-figure source: [Natural Language Processing (almost) from Scratch (Collobert et al., 2011)](https://arxiv.org/abs/1103.0398v1.pdf)\n",
    "- right-figure source: [Bilingual Word Embeddings for Phrase-Based Machine Translation (Socher et al., 2013, EMNLP)](https://ai.stanford.edu/~wzou/emnlp2013_ZouSocherCerManning.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "In Natural language processing, Named Entity Recognition (NER) is a process where a sentence or a chunk of text is parsed through to find entities that can be put under categories like names, organizations, locations, quantities, monetary values, percentages, etc. Traditional NER algorithms included only names, places, and organizations.\n",
    "\n",
    "Since the embeddings can capture word senses etc., it is practical and beneficial to use word embeddings in NER task. Embeddings can capture entity informations while capturing word relations.\n",
    "\n",
    "<p float=\"left\">\n",
    "    <img src=\"ner2.png\" width=\"700\" /> \n",
    "  <img style=\"padding: 40px\" src=\"ner1.png\" width=\"600\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "figure sources: [link](https://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To learn word embeddings, huge size of training data is always useful. For example GloVe is trained on 5 separate corpora:\n",
    "    * 2010 Wikipedia dump with 1 billion tokens\n",
    "    * 2014 Wikipedia dump with 1.6 billion tokens\n",
    "    * Gigaword 5 which has 4.3 billion tokens\n",
    "    * the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens\n",
    "    * 42 billion tokens of web data, from Common Crawl\n",
    "<br><br>\n",
    "\n",
    "- You can download pre-trained word embeddings online.\n",
    "    * [GloVe pre-trained vectors](https://nlp.stanford.edu/projects/glove/)\n",
    "        * Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download)\n",
    "        * Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)\n",
    "        * Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download)\n",
    "    * [Hellinger PCA vectors](http://lebret.ch/words/)\n",
    "    * [word2vec pre-trained vectors](https://wikipedia2vec.github.io/wikipedia2vec/pretrained/)\n",
    "    * etc.\n",
    "    <br><br>\n",
    "- Transfer embedding to new task with smaller training set.\n",
    "    * Language Modelling\n",
    "    * Predictive Typing\n",
    "    * Spelling/Grammar Correction\n",
    "    * Summarization\n",
    "    * NMT\n",
    "    * etc.\n",
    "<br><br>\n",
    "\n",
    "- Finetune the word embeddings with new data (if your training data relatively big).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing\n",
    "A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between \"head\" words and words which modify those heads. The figure below shows a dependency parse of a short sentence.\n",
    "\n",
    "Syntactic Parsing or Dependency Parsing is the task of recognizing a sentence and assigning a syntactic structure to it. The most widely used syntactic structure is the parse tree which can be generated using some parsing algorithms. These parse trees are useful in various applications like grammar checking or more importantly it plays a critical role in the semantic analysis stage.\n",
    "\n",
    "Dependency parsing is the task of analyzing the syntactic dependency structure of a given input sentence $S$. The output of a dependency parser is a dependency tree where the words of the input sentence are connected by typed dependency relations. Formally, the dependency parsing problem asks to create a mapping from the input sentence with words $S = w_0w_1...w_n$ (where $w_0$ is the ROOT) to its dependency tree graph $G$.\n",
    "\n",
    "<p float=\"left\">\n",
    "   <img style=\"padding: 80px\" src=\"dp.png\" width=\"500\" /> \n",
    "  <img style=\"padding: 40px\" src=\"dp2.png\" width=\"600\" />\n",
    "</p>\n",
    "\n",
    "[A Fast and Accurate Dependency Parser using Neural Networks (Chen et al., 2014)](https://www.aclweb.org/anthology/D14-1082/)\n",
    "\n",
    "right-figure source [CS224n Part IV](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes04-dependencyparsing.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various tasks can be modeled with representations. Since words can be represented by embeddings, images can be (or even signals/audio) represented by a latent space $z$. Autoencoders is a way to learn latent representation of data.\n",
    "<div style=\"text-align:center\">\n",
    "\n",
    "</div>\n",
    "<p float=\"left\">\n",
    "    <img style=\"padding: 40px\" src=\"ae3.png\" width=\"500\" /> \n",
    "   <img style=\"padding: 40px\" src=\"ae1.png\" width=\"450\" /> \n",
    "  <img style=\"padding: 40px\" src=\"ae2.png\" width=\"450\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Properties Of Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings have very important property: analogies. Analogy is another semantic property of embeddings that can capture relational meanings. Simply in words, analogy is to find **X**:\n",
    "\n",
    "- **A is to B as C is to X**\n",
    "\n",
    "For example **“woman is to queen as man is to X**. In this example **X** should be the word *king*.\n",
    "\n",
    "Interestingly, such embeddings exhibit seemingly linear behaviour in analogies. This linear behaviour can be formulated as\n",
    "\n",
    "- $w_a$ is to $w_a'$ as $w_b$ is to $w_b'$ $\\rightarrow \\rightarrow \\rightarrow$ $w_a' - w_a + w_b \\approx w_b'$\n",
    "\n",
    "- vec('*queen*') - vec('*woman*') + vec('*man*') $\\approx$ vec('*king*')\n",
    "\n",
    "or\n",
    "\n",
    "- vec('*queen*') - vec('*woman*') $\\approx$ vec('*king*') - vec('*man*')\n",
    "\n",
    "\n",
    "Another example:\n",
    "\n",
    "- vec('*Paris*') - vec('*France*') $\\approx$ vec('*Italy*') - vec('*Rome*')\n",
    "\n",
    "\n",
    "\n",
    "<p float=\"left\">\n",
    "   <img style=\"padding: 80px\" src=\"mikolov1.png\" width=\"200\" /> \n",
    "  <img style=\"padding: 30px\" src=\"analogy1.png\" width=\"500\" />\n",
    "     <img style=\"padding: 50px\" src=\"mikolov2.png\" width=\"600\" />\n",
    "</p>\n",
    "\n",
    "- left-figure source: [Linguistic Regularities in Continuous Space Word Representations (Mikolov et al., 2013)](https://www.aclweb.org/anthology/N13-1090.pdf)\n",
    "- mid-figure source: [Speech and Language Processing, Daniel Jurafsky, Third Edition](https://web.stanford.edu/~jurafsky/slp3/)\n",
    "- right-figure source: [Efficient Estimation of Word Representations in Vector Space (Mikolov etl al., 2013)](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "Do vector embeddings capture syntactic relationships? Yes. Capture with linear behaviours? Yes.\n",
    "\n",
    "\n",
    "<p float=\"left\">\n",
    "   <img style=\"padding: 80px\" src=\"syntax2.png\" width=\"200\" /> \n",
    "  <img style=\"padding: 30px\" src=\"syntax1.png\" width=\"500\" />\n",
    "</p>\n",
    "\n",
    "- left-figure source: [Linguistic Regularities in Continuous Space Word Representations (Mikolov et al., 2013)](https://www.aclweb.org/anthology/N13-1090.pdf)\n",
    "- right-figure source: [Speech and Language Processing, Daniel Jurafsky, Third Edition](https://web.stanford.edu/~jurafsky/slp3/)\n",
    "\n",
    "\n",
    "For more formal definition of analogy? Check [Analogies Explained: Towards Understanding Word Embeddings (Allen et al., 2019)](https://arxiv.org/pdf/1901.09813.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extrinsic Evaluation\n",
    "    * This is the evaluation on a real task.\n",
    "    * Can be slow to compute performance.\n",
    "    * Unclear if the subsystem is the problem, or our system.\n",
    "    * If replacing subsystem improves performance, the change is likely good.\n",
    "    * NER, Question Answering etc.\n",
    "\n",
    "\n",
    "- Intrinsic Evaluation\n",
    "    * Fast to compute\n",
    "    * Helps to understand subsystem\n",
    "    * Needs positive correlation with real task to determine usefulness\n",
    "    \n",
    "    \n",
    "SimLex-999 dataset (Hill et al., 2015) gives values on a scale from 0 to 10 by asking humans to judge how similar one word is to another. Other datasets for evaluating word vectors: \n",
    "- WordSim 353 \n",
    "- TOEFL Dataset\n",
    "- SCWS Dataset\n",
    "- Word-in-Context (WiC)\n",
    "- Miller & Charles Dataset\n",
    "- Rubenstein & Goodenough Dataset\n",
    "- Stanford Rare Word (RW)\n",
    "\n",
    "<p float=\"left\">\n",
    "   <img src=\"glove-table.png\" width=\"400\" /> \n",
    "  <img style=\"padding: 100px\" src=\"glove-table2.png\" width=\"400\" />\n",
    "</p>\n",
    "\n",
    "Some evaluation metrics from [GloVe: Global Vectors for Word Representation (Pennington et al., 2014)](https://www.aclweb.org/anthology/D14-1162.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix\n",
    "Embedding matrix can be represented as a single matrix $E \\in \\mathbb{R}^{d \\times \\mid V \\mid}$ (or $E \\in \\mathbb{R}^{ \\mid V \\mid \\times d }$, it does not even matter), where d is embedding size and $\\mid V \\mid$ is size of the vocabulary $V$.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$E = \\begin{bmatrix}\n",
    "\\text{hello} & \\text{i} & \\text{love} & \\text{inzva} & \\cdots & \\text{sanctuary}\\\\\n",
    "0.1 & 0.137 & -0.03 & -0.44 & \\cdots & 0.36\\\\\n",
    "-0.78 & -0.25 & 2.09 & 0.19 & \\cdots & 0.32\\\\\n",
    "-3.1 & 1.54 & -2.52 & 0.2 & \\cdots & -1.51\\\\\n",
    "1.13 & -0.78 & -0.56 & 0.95 & \\cdots & 0.2112\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "- 0.6 & 0.13 & 3.89 & -0.071 & -0.27 & 0.27 \n",
    "\\end{bmatrix} \\in \\mathbb{R}^{d \\times \\mid V \\mid}$$\n",
    "\n",
    "But how to learn them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Neural Probabilistic Language Model (Bengio et al., 2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In neural language models, the prior context is represented by embeddings of the previous words*. Representing the prior context as embeddings, rather than by exact words as used in [n-gram](https://web.stanford.edu/~jurafsky/slp3/3.pdf) language models, allows neural language models to generalize to unseen data much better than n-gram language models.\n",
    "\n",
    "For example in our training set we see this sentence,\n",
    "\n",
    "    Today, after school, I am planning to go to cinema.\n",
    "    \n",
    "but we have never seen the word \"concert\" after the words \"go to\". In our test set we are trying to predict what comes after the prefix \"Today, after school, I am planning to go to\". An n-gram language model will predict \"cinema\" but not \"concert\". But a neural language model, which can make use of the fact that \"cinema\" and \"concert\" have similar embeddings, will be able to assign a reasonably high probability to \"concert\" as well as \"cinema\", merely because they have similar vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2003 Bengio proposed a neural language model that can learns and uses embeddings to predict the next word in a sentence. Formally representation, for a sequence of words\n",
    "\n",
    "$$x^{(1)}, x^{(2)}, ..., x^{(t)}$$\n",
    "\n",
    "the probability distribution of the next word (output) is\n",
    "\n",
    "$$p(x^{(t+1)} \\mid x^{(1)}, x^{(2)}, ..., x^{(t)})$$\n",
    "\n",
    "Bengio proposed a fixed-window neural Language Model which can be seen as the same approach of n-grams.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "\"<s> as the proctor started the clock</s> the students opened their ____.\"\n",
    "</div>\n",
    "<br>\n",
    "We have a moving window at time $t$ with an embedding vector representing each of the window size previous words. For window size 3, words $w_{t-1}, w_{t-2}, w_{t-3}$. These 3 vectors are concatenated together to produce input x. The task is to predict $w_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "   <img src=\"neural.png\" width=\"400\" /> \n",
    "  <img style=\"padding: 40px\" src=\"neural2.png\" width=\"800\" />\n",
    "</p>\n",
    "\n",
    "- left-figure source: [CS224n lecture 5](http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture05-rnnlm.pdf)\n",
    "- right-figure source: [Speech and Language Processing, Daniel Jurafsky, Third Edition](https://web.stanford.edu/~jurafsky/slp3/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we are going to represent each of the $N$ prevuios words as a one-hot-vector of length $\\mid V \\mid$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward equation for neural language model\n",
    "\n",
    "- Input $x_i \\in R^{1 \\times \\mid V \\mid}$\n",
    "\n",
    "- Learning word embeddings: $e = concat(x_1 E^T, x_2 E^T, x_3 E^T)$\n",
    "    * $E \\in \\mathbb{R}^{d \\times \\mid V \\mid}$\n",
    "    * $e \\in \\mathbb{R}^{1 \\times 3d}$\n",
    "    \n",
    "    \n",
    "- $h = \\sigma(e W^T + b_1)$\n",
    "    * $W \\in \\mathbb{R}^{d_h \\times 3d}$\n",
    "    * $h \\in \\mathbb{R}^{1 \\times d_h}$\n",
    "    \n",
    "    \n",
    "- $z = h U^T + b_2$\n",
    "    * $U \\in \\mathbb{R}^{\\mid V \\mid \\times d_h}$\n",
    "    * $z \\in \\mathbb{R}^{1 \\times \\mid V \\mid}$\n",
    "    \n",
    "    \n",
    "- $\\hat{y} = softmax(z) \\in \\mathbb{R}^{1 \\times \\mid V \\mid}$\n",
    "\n",
    "\n",
    "Then the model is trained, at each word $w_t$, the negative log likelihood loss is:\n",
    "\n",
    "$$ L = - \\log p(w_t \\mid w_{(t-1)}, w_{(t-2)}, ..., w_{(t-n+1)}) = softmax(z) $$\n",
    "\n",
    "<br>\n",
    "$$ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\partial - \\log p(w_t \\mid w_{(t-1)}, w_{(t-2)}, ..., w_{(t-n+1)})}{\\partial \\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec is proposed in the paper called [Distributed Representations of Words and Phrases and their Compositionality (Mikolov et al., 2013)](https://arxiv.org/abs/1310.4546). It allows you to learn the high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. Word2vec algorithm uses Skip-gram [Efficient Estimation of Word Representations in Vector Space\n",
    " (Mikolov et al., 2013)](https://arxiv.org/abs/1301.3781) model to learn efficient vector representations. Those learned word vectors has interesting property, words with semantic and syntactic affinities give the necessary result in mathematical similarity operations.\n",
    " \n",
    "Suppose that you have a sliding window of a fixed size moving along a sentence: the word in the middle is the “target” and those on its left and right within the sliding window are the context words.\n",
    "\n",
    "The skip-gram model is trained to predict the probabilities of a word being a context word for the given target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "   <img style=\"padding: 40px\" src=\"sent1.png\" width=\"600\" /> \n",
    "  <img style=\"padding: 40px\" src=\"sent2.png\" width=\"600\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example consider this sentence,\n",
    "\n",
    "         \"A change in Quantity also entails a change in Quality\"\n",
    "Our target and context pairs for window size of 5:\n",
    "\n",
    "\n",
    "| Sliding window (size = 5)       | Target word       |  Context |\n",
    "| -----------                     | -----------       |  -----------       |\n",
    "| \\[A change in\\]                 | a                 |  change, in            |\n",
    "| \\[A change in Quantity \\]       | change        |  a, in, quantitiy             |\n",
    "| \\[A change in Quantity  also\\]       | in  | a, change, quantitiy,also               |\n",
    "| ...      |  ...      |  ...          |\n",
    "| \\[entails a change in Quality\\]      | change    |  entails, a, in, Quality             |\n",
    "| \\[a change in Quality\\]              | in   |  a, change, Quality             |\n",
    "| \\[change in Quality\\]              | quality   |  change, in             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each context-target pair is treated as a new observation in the data. \n",
    "\n",
    "For each position $t=1,..,T$ predict context words within a window of fixed size $m$, given center word $w_j$. In Skip-gram connections we have an objective to maximize, likelihood (or minimize log-likelihood):\n",
    "\n",
    "<br>\n",
    "$$\\max \\limits_{\\theta} \\prod_{\\text{center}} \\prod_{\\text{context}} p(\\text{context}|\\text{center} ;\\theta)$$\n",
    "\n",
    "<br>\n",
    "$$= \\max \\limits_{\\theta} \\prod_{t=1}^T \\prod_{-c \\leq j \\leq c, j \\neq c} p(w_{t+j}|w_t; \\theta)$$\n",
    "\n",
    "<br>\n",
    "$$= \\min \\limits_{\\theta} -\\frac{1}{T} \\prod_{t=1}^T \\prod_{-c \\leq j \\leq c, j \\neq c} p(w_{t+j}|w_t; \\theta)$$\n",
    "\n",
    "<br>\n",
    "$$= \\min \\limits_{\\theta} -\\frac{1}{T} \\sum_{t=1}^T \\sum_{-c \\leq j \\leq c, j \\neq c} \\log p(w_{t+j}|w_t; \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, How can we calculate those probabilities? Softmax gives the normalized probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterization Of Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say $w_t$ is our target word and $w_c$ is current context word. The softmax is defined as\n",
    "\n",
    "<br>\n",
    "$$p(w_c \\mid w_t) = \\frac{\\exp(v_{w_c}^T v_{w_t})}{\\sum_{i=0}^{\\mid V \\mid} \\exp(v_{w_i}^T v_{w_t})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maximizing this log-likelihood function under $v_{w_t}$ gives you the most likely value of the $v_{w_t}$ given the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "$$ \\frac{\\partial}{\\partial v_{w_t}}\\cdot \\log \\frac{\\exp(v_{w_c}^T v_{w_t})}{\\sum_{i=0}^{\\mid V \\mid} \\exp(v_{w_i}^T v_{w_t})}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ = \\frac{\\partial}{\\partial v_{w_t}}\\cdot \\log \\underbrace{\\exp(v_{w_c}^T v_{w_t})}_{\\text{numerator}} - \\frac{\\partial}{\\partial v_{w_t}}\\cdot \\log \\underbrace{\\sum_{i=0}^{\\mid V \\mid} \\exp(v_{w_i}^T v_{w_t})}_{\\text{denominator}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial v_{w_t}} \\cdot v_{w_c}^T v_{w_t} =  v_{w_c} \\; \\; (\\text{numerator})$$\n",
    "\n",
    "Now, it is time to derive denominator.\n",
    "<br>\n",
    "\n",
    "$$\\frac{\\partial}{\\partial v_{w_t}}\\cdot \\log \\sum_{i=0}^{\\mid V \\mid} \\exp(v_{w_i}^T v_{w_t}) = \\frac{1}{\\sum_{i=0}^{\\mid V \\mid} \\exp(v_{w_i}^T v_{w_t})} \\cdot \\frac{\\partial}{\\partial v_{w_t}} \\sum_{i=0}^{\\mid V \\mid} \\exp(v_{w_i}^T v_{w_t})$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ = \\frac{1}{\\sum_{i=0}^{\\mid V \\mid} \\exp(v_{w_i}^T v_{w_t})} \\cdot  \\sum_{i=0}^{\\mid V \\mid} \\frac{\\partial}{\\partial v_{w_t}} \\cdot \\exp(v_{w_i}^T v_{w_t})$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ = \\frac{1}{\\sum_{i=0}^{\\mid V \\mid} \\exp(v_{w_i}^T v_{w_t})} \\cdot  \\sum_{i=0}^{\\mid V \\mid} \\exp(v_{w_i}^T v_{w_t}) \\frac{\\partial}{\\partial v_{w_t}} v_{w_i}^T v_{w_t}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ = \\frac{\\sum_{i=0}^{\\mid V \\mid} \\exp(v_{w_i}^T v_{w_t}) \\cdot v_{w_i}}{\\sum_{i=0}^{\\mid V \\mid} \\exp(v_{w_i}^T v_{w_t})} \\;\\; (\\text{denominator}) $$\n",
    "\n",
    "\n",
    "To sum up,\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_t} \\log p(w_c \\mid w_t) = v_{w_c} - \\frac{\\sum_{j=0}^{\\mid V \\mid} \\exp(v_{w_j}^T v_{w_t}) \\cdot v_{w_j}}{\\sum_{i=0}^{\\mid V \\mid} \\exp(v_{w_i}^T v_{w_t})}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ = v_{w_c} - \\sum_{j=0}^{\\mid V \\mid} \\frac{\\exp(v_{w_j}^T v_{w_t})}{\\sum_{i=0}^{\\mid V \\mid} \\exp(v_{w_i}^T v_{w_t})} \\cdot v_{w_j}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ \\underbrace{= v_{w_c} - \\sum_{j=0}^{\\mid V \\mid} p(w_j \\mid w_t) \\cdot v_{w_j}}_{\\nabla_{w_t}\\log p(w_c \\mid w_t)}$$\n",
    "\n",
    "This is the observed representation subtract $\\mathop{\\mathbb{E}}[w_j \\mid w_t]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling (Noise Contrastive Estimation (NCE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Noise Contrastive Estimation (NCE) metric intends to differentiate the target word from noise samples using a logistic regression classifier [(Noise-contrastive estimation: A new estimation principle for unnormalized statistical models, Gutmann et al., 2010)](http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf). \n",
    "\n",
    "In softmax computation, look at the denominator. The summation over $\\mid V\\mid$ is computationally expensive. The training or evaluation takes asymptotically $O(\\mid V \\mid)$. In a very large corpora, the most frequent words can easily occur hundreds or millions of times (\"in\", \"and\", \"the\", \"a\" etc.). Such words provides less information value than the rare words. For example, while the skip-gram model benefits from observing co-occurences of \"inzva\" and \"deep learning\", it benefits much less from observing the frequent co-occurences of \"inzva\" and \"the\".In a very large corpora, the most frequent words can easily occur hundreds or millions of times (\"in\", \"and\", \"the\", \"a\" etc.). Such words provides less information value than the rare words. For example, while the skip-gram model benefits from observing co-occurences of \"inzva\" and \"deep learning\", it benefits much less from observing the frequent co-occurences of \"inzva\" and \"the\". \n",
    "\n",
    "For every training step, instead of looping over the entire vocabulary, we can just sample several negative examples! We \"sample\" from\n",
    "a noise distribution $P_n(w)$ whose probabilities match the ordering of the frequency of the vocabulary.\n",
    "\n",
    "Consider a pair $(w_t, w_c)$ of word and context. Did this pair come from the training data? Let’s denote by $p(D=1 \\mid w_t,w_c)$ the probability that $(w_t, w_c)$ came from the corpus data. Correspondingly $p(D=0 \\mid w_t,w_c)$ will be the probability that $(w_t, w_c)$ didn't come from the corpus data. First, let’s model $p(D=1 \\mid w_t,w_c)$ with sigmoid:\n",
    "<br>\n",
    "\n",
    "$$p(D=1 \\mid w_t,w_c) = \\sigma(v_{w_c}^T v_{w_t}) = \\frac{1}{1 + \\exp(- v_{w_c}^T v_{w_t})}$$\n",
    "\n",
    "Now, we build a new objective function that tries to maximize the probability of a word and context being in the corpus data if it indeed is, and maximize the probability of a word and context not being in the corpus data if it indeed is not. Maximum likelihood says:\n",
    "\n",
    "$$ \\max \\prod_{(w_t, w_c) \\in D} p(D=1 \\mid w_t,w_c) \\times \\prod_{(w_t, w_c) \\in D'} p(D=0 \\mid w_t,w_c)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ = \\max \\prod_{(w_t, w_c) \\in D} p(D=1 \\mid w_t,w_c) \\times \\prod_{(w_t, w_c) \\in D'} 1 - p(D=1 \\mid w_t,w_c)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ = \\max \\sum_{(w_t, w_c) \\in D} \\log p(D=1 \\mid w_t,w_c) + \\sum_{(w_t, w_c) \\in D'}  \\log (1 - p(D=1 \\mid w_t,w_c))$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ = \\max \\sum_{(w_t, w_c) \\in D} \\log \\frac{1}{1 + \\exp(- v_{w_c}^T v_{w_t})} + \\sum_{(w_t, w_c) \\in D'} \\log \\left(1 - \\frac{1}{1 + \\exp(- v_{w_c}^T v_{w_t})}\\right)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Note that $\\frac{\\exp(-x)}{(1 + \\exp(-x))} \\times \\frac{\\exp(x)}{\\exp(x)} = \\frac{1}{(1 + \\exp(x))}$\n",
    "\n",
    "\n",
    "$$ = \\max \\sum_{(w_t, w_c) \\in D} \\log \\frac{1}{1 + \\exp(- v_{w_c}^T v_{w_t})} + \\sum_{(w_t, w_c) \\in D'} \\log \\frac{1}{1 + \\exp(v_{w_c}^T v_{w_t})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximizing the likelihood is the same as minimizing the negative log likelihood:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$L = - \\sum_{(w_t, w_c) \\in D} \\log \\frac{1}{1 + \\exp(- v_{w_c}^T v_{w_t})} -  \\sum_{(w_t, w_c) \\in D'} \\log \\frac{1}{1 + \\exp(v_{w_c}^T v_{w_t})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $D'$ is a \"false\" or \"negative\" corpus. Where we would have sentences like \"the school is eaten by pilgrims\". Unnatural sentences that should get a low probability of ever occurring. We can generate $D'$ on the fly by randomly sampling this negative from the word bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Negative Sampling (NEG) proposed in the original word2vec paper. NEG approximates the binary classifier’s output with sigmoid functions as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(d=1 \\vert v_{w_c}, v_{w_t}) &= \\sigma(v_{w_c}^T v_{w_t}) \\\\\n",
    "p(d=0 \\vert v_{w_c}, v_{w_t}) &= 1 - \\sigma(v_{w_c}^T v_{w_t}) = \\sigma(-v_{w_c}^T v_{w_t})\n",
    "\\end{align}$$\n",
    "\n",
    "So the objective is\n",
    "\n",
    "$$L = - [ \\log \\sigma(v_{w_c}^T v_{w_t}) +  \\sum_{\\substack{i=1 \\\\ \\tilde{w}_i \\sim Q}}^K \\log \\sigma(v_{\\tilde{w}_i}^T v_{w_t})]$$\n",
    "\n",
    "In the above formulation, ${v_{\\tilde{w}_i} \\mid i = 1 . . . K}$ are sampled from $P_n(w)$. How to define $P_n(w)$? In the word2vec paper $P_n(w)$ defined as \n",
    "\n",
    "$$P_n(w_i) = 1 - \\sqrt{\\frac{t}{freq(w_i)}} \\;\\; t \\approx 10^{-5}$$\n",
    "\n",
    "This distribution assigns lower probability for lower frequency words, higher probability for higher frequency words.\n",
    "\n",
    "Hence, this distribution is sampled form a unigram distribution $U(w)$ raised to the  $\\frac{3}{4}$rd power.\n",
    "\n",
    "The unigram distribuiton is defined as\n",
    "\n",
    "$$P_n(w)= \\left(\\frac{U(w)}{Z}\\right)^\\alpha$$\n",
    "\n",
    "Or just by Andrew NG's definition:\n",
    "\n",
    "$$P_n(w_i) = \\frac{freq(w_i)^\\frac{3}{4}}{\\sum_{j=0}^M freq(w_j)^\\frac{3}{4}}$$\n",
    "\n",
    "Raising the unigram distribution $U(w)$ to the power of $\\alpha$ has an effect of smoothing out the distribution. It attempts to combat the imbalance between common words and rare words by decreasing the probability of drawing common words, and increasing the probability drawing rare words.\n",
    "\n",
    "<div style=\"text-align:center\"> <img src=\"noise_dist.png\" width=\"800\" height=\"800\"> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "unig_dist  = {'inzva': 0.023, 'deep': 0.12, 'learning': 0.34, 'the': 0.517}\n",
    "sum(unig_dist.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inzva': 0.044813853132981724,\n",
       " 'deep': 0.15470428538870049,\n",
       " 'learning': 0.33785130228003507,\n",
       " 'the': 0.4626305591982827}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha      = 3 / 4\n",
    "noise_dist = {key: val ** alpha for key, val in unig_dist.items()}\n",
    "Z = sum(noise_dist.values())\n",
    "noise_dist_normalized = {key: val / Z for key, val in noise_dist.items()}\n",
    "noise_dist_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(noise_dist_normalized.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'the', 'deep', 'learning', 'the', 'learning', 'the',\n",
       "       'inzva', 'the', 'learning'], dtype='<U8')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 10\n",
    "np.random.choice(list(noise_dist_normalized.keys()), size=K, p=list(noise_dist_normalized.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on negative sampling? [link](https://www.aclweb.org/anthology/D17-1037.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deriving negative sampling loss function is trivial. Left to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hierarchical softmax is proposed to make the sum calculation faster with the help of a binary tree structure. The model uses a binary tree to represent all words in the\n",
    "vocabulary. The $\\mid V \\mid$ words must be leaf units of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
